{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juan Manuel Deutsch, Cesar Felipe Giraldo, Julian Felipe Pulido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio tenemos $X$ siendo la matriz base y $Σ$ como la matriz de covarianza de $X$\n",
    "\n",
    "Tambien tenemos los componentes principales $u_1, u_2,...,u_n$, que son los autovectores o vectores propios de $Σ$ que tienen varianzas asociadas $u_i^TΣu_i$, donde $u_i^T$ es la transpuesta del vector propio $u_i$\n",
    "\n",
    "Ahora tendremos una tranformación lineal de los datos $X$, siendo $AX$ donde $A$ es una transformación $m x m$\n",
    "\n",
    "Ahora con la matriz transformada tenemos la matriz de covarianza transformada $AΣA^T$ cuyos componentes principales estan dados por $v_1, v_2,...,v_n$ estos siendo los autovectores cuyas varianza asociadas estan dadas por $v_i^TAΣA^Tv_i$\n",
    "\n",
    "Teniendo en cuenta que necesitamos demostrar los componentes principales y como cambian tenemos con relacion a $AX$ y $X$\n",
    "\n",
    "$u_1$ siendo componente principal cuya varianza asociada es $u_i^TΣu_i$ y siendo $v_1$ el componente principal cuya varianza asociada es $v_i^TAΣA^Tv_i$\n",
    "\n",
    "Teniendo en cuenta que $u_1$ y $v_1$ estan relacionados por una transformación lineal tenemos que $v_1 = Tu_1$ para una matriz T\n",
    "\n",
    "Ahora, se puede escribir la varianza asociada de $v_1$ como:\n",
    "\n",
    "$$v_1^TAΣA^Tv_1 = (Tu_1)^TAΣA^T(Tu_1) = u_1^TT^TAΣA^TTu_1$$\n",
    "\n",
    "Dado que $T$ es una matriz de transformacion lineal, $T^TAΣA^TT$ es otra matriz de covarianza $Σ'$\n",
    "\n",
    "Por lo tanto ahora tenemos:\n",
    "\n",
    "$$v_1^TAΣA^Tv_1 = u_1^TΣ'u_1$$\n",
    "\n",
    "Concluyendo con esta demostración, tenemos que la varianza asociada $v_1$ depende de la matriz de covarianza $Σ'$ que es una version transformada de la matriz de covarianza original $Σ$, por lo cual, $Σ'$ es diferente a $Σ$, entonces la varianza asociada a estos van a ser diferentes, por lo que un valor propio esta ligado conjunto de datos original y el otro valor propio esta ligado al conjunto transformado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta demostracion tenemos $D$, como una matriz $f x c$ donde $f$ son filas y $c$ son columnas, ademas de $D'$ que es la matriz transformada con una transformación afin a una o mas columnas\n",
    "\n",
    "Iniciando calculamos la matriz de correlación $corr(D)$\n",
    "$$corr(D) = 1/n (D - D_u)^T(D - D_u)$$\n",
    "Donde $D_u$ es la media de cada columna de $D$\n",
    "De acuerdo a eso se los componentes principales por medio de $corr(D)$\n",
    "\n",
    "Ahora realizamos el mismo proceso con la tranformación afin, que seria del siguiente modo:\n",
    "\n",
    "$$D' = A*D+b$$\n",
    "\n",
    "Donde $A$ es la transformación lineal y $b$ es el desplazamiento\n",
    "\n",
    "Ahora calculamos la correlacion \n",
    "\n",
    "$$corr(D') = 1/n (D' - D'_u)^T(D' - D'_u)$$\n",
    "\n",
    "Donde ahora $D'_u$ es la media de la matriz transformada, donde las columnas de $D'$ se ven afectadas por la transformación afin\n",
    "\n",
    "Pero debemos tener en cuenta que para la covarianza los efectos de una transformación afin se dan de la siguiente forma:\n",
    "\n",
    "$$cov(D') = A^Tcov(D)A$$\n",
    "\n",
    "No obstante, estamos tratando es con matrices de correlación, las cuales son versiones escaladas de las matrices de covariaza, por lo cual la matriz de covarianza tambien afecta a la matriz de correlación con la misma transformación afin\n",
    "\n",
    "Por lo cual los autovectores o vectores propios de $corr(D')$ son los mismos que $corr(D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos reducidos A:\n",
      " [[ 2.22383293  6.62077922  0.9864382  -0.03164106]\n",
      " [ 5.55127862 -4.7594466  -0.5427392  -2.28080035]\n",
      " [-0.55058985 -0.98729531  1.77065664  0.80993837]\n",
      " [-3.68276669  1.36321497  3.73620713 -3.34999953]\n",
      " [ 4.37500465  2.74874794 -3.09709231 -0.07380571]\n",
      " [-2.10915442  0.84383508 -2.99845749  2.2964254 ]\n",
      " [-3.03064114  0.41677691 -2.57421906 -1.72366106]\n",
      " [-6.92241058 -2.45359706 -1.51927006  0.63565965]\n",
      " [ 1.04675051 -0.9551932   4.22672217  3.08842427]\n",
      " [ 3.09869596 -2.83782195  0.01175398  0.62946002]]\n",
      "\n",
      "La varianza de la matriz A: 37.02020528711315\n",
      "La varianza de la matriz Total: 39.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def PCA(D, red):\n",
    "    mu = np.mean(D, axis=0)\n",
    "    Z = D - mu\n",
    "    cov_matrix = np.cov(Z, rowvar=False, bias=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "    topEigenvalues = eigenvalues[sorted_indices]\n",
    "    topEigenVector = eigenvectors[:,sorted_indices]\n",
    "    cumvariance = np.cumsum(topEigenvalues)\n",
    "    \n",
    "    if red < 1:\n",
    "        total_variance = np.sum(topEigenvalues)\n",
    "        cumvariance = np.cumsum(topEigenvalues)\n",
    "        variance_top = red * total_variance\n",
    "        numdims = np.argmax(cumvariance >= variance_top) + 1\n",
    "    else:\n",
    "        numdims = red\n",
    "    top_eigenvectors = topEigenVector[:, :numdims] \n",
    "    total_variance = np.sum(topEigenvalues[:numdims]) \n",
    "    variance = total_variance / np.sum(topEigenvalues)\n",
    "    A = np.dot(Z, top_eigenvectors)\n",
    "\n",
    "    v = np.sum(topEigenvalues[:numdims])\n",
    "    varianceTotal = np.sum(topEigenvalues)      \n",
    "\n",
    "\n",
    "    \n",
    "    return A, v, varianceTotal\n",
    "\n",
    "D = np.array([[4, 1, 0, 9, 8],\n",
    " [9, 9, 1, 6, 1],\n",
    " [4, 5, 3, 3, 5],\n",
    " [6, 2, 7, 3, 8],\n",
    " [5, 1, 0, 8, 1],\n",
    " [0, 4, 4, 6, 3],\n",
    " [3, 3, 7, 6, 3],\n",
    " [0, 6, 9, 3, 4],\n",
    " [4, 5, 0, 1, 6],\n",
    " [6, 5, 1, 3, 1]])\n",
    "\n",
    "A, variance, varTot = PCA(D, 4)\n",
    "print(f'Datos reducidos A:\\n {A}\\n')\n",
    "print(f'La varianza de la matriz A: {variance}')\n",
    "print(f'La varianza de la matriz Total: {varTot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/4b01rf9j7cl6mcd3652m260w0000gn/T/ipykernel_92233/2984195183.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "iris_data = pd.read_csv('iris.csv')\n",
    "baseball_data = pd.read_csv('baseball_numeric.csv')\n",
    "fabert_data = pd.read_csv('fabert.csv')\n",
    "amazon_data = pd.read_csv('amazon.csv')\n",
    "\n",
    "\n",
    "adf2 = amazon_data.drop('Class', axis=1)\n",
    "fdf2 = fabert_data.drop('class', axis=1)\n",
    "idf2 = iris_data.drop('species', axis=1)\n",
    "bndf = baseball_data\n",
    "\n",
    "adf3 = amazon_data.drop('Class', axis=1).values\n",
    "fdf3 = fabert_data.drop('class', axis=1).values\n",
    "idf3 = iris_data.drop('species', axis=1).values\n",
    "bndf3 = baseball_data.values\n",
    "\n",
    "datasets = [bndf3, fdf3, idf3, adf3]\n",
    "data_dict = ['Baseball Dataset', 'Fabert Dataset', 'Iris Dataset', 'Amazon Dataset']\n",
    "\n",
    "datasetsDictionary = {'Iris': iris_data, 'Baseball Numeric': baseball_data, 'Fabert': fabert_data, 'Amazon': amazon_data}\n",
    "datasetsDictionary2 = {'Iris': idf2, 'Basebal Numeric': bndf, 'Fabert': fdf2, 'Amazon': adf2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nuestra función</th>\n",
       "      <th>Función de numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseball Dataset</th>\n",
       "      <td>0.011873</td>\n",
       "      <td>0.000933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fabert Dataset</th>\n",
       "      <td>3.412953</td>\n",
       "      <td>2.590314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iris Dataset</th>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon Dataset</th>\n",
       "      <td>8.014229</td>\n",
       "      <td>6.139668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Nuestra función  Función de numpy\n",
       "Baseball Dataset         0.011873          0.000933\n",
       "Fabert Dataset           3.412953          2.590314\n",
       "Iris Dataset             0.000178          0.000139\n",
       "Amazon Dataset           8.014229          6.139668"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def getStats(A):\n",
    "    start = timeit.default_timer()\n",
    "    #Media\n",
    "    mu = list(sum(row)/len(row) for row in zip(*A))\n",
    "    #Covarianza\n",
    "    n = A.shape[0]\n",
    "    cov_mu = A - mu\n",
    "    cov = np.dot(cov_mu.T, cov_mu) / (n)\n",
    "    return mu, cov, timeit.default_timer() - start\n",
    "\n",
    "def getStatsNumpy(A):\n",
    "    start = timeit.default_timer()\n",
    "    mu = np.mean(A, axis=0)\n",
    "    cov = np.cov(A, rowvar=False, bias=False)\n",
    "    return mu, cov, timeit.default_timer() - start\n",
    "\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    combined_results = np.array([getStats(dataset)[2], getStatsNumpy(dataset)[2]]) \n",
    "    all_results.append(combined_results)\n",
    "\n",
    "df = pd.DataFrame(all_results, index = data_dict, columns=['Nuestra función', 'Función de numpy'])\n",
    "display(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Dataset  Dimensiones  Varianza total absoluta  \\\n",
      "0             Iris            1                 4.224841   \n",
      "1  Basebal Numeric            2             17228.155686   \n",
      "2           Fabert          516                 2.509025   \n",
      "3           Amazon          197             12155.426523   \n",
      "\n",
      "   Varianza Total Relativa  \n",
      "0                 0.924616  \n",
      "1                 0.989389  \n",
      "2                 0.902976  \n",
      "3                      inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/4b01rf9j7cl6mcd3652m260w0000gn/T/ipykernel_92233/3154555406.py:13: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  relative_total_variance = variance / total_variance_original\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "results = []\n",
    "\n",
    "for dataset_name, dataset in datasetsDictionary2.items():\n",
    "    \n",
    "    numeric_dataset = dataset.select_dtypes(include=[float])\n",
    "    A = numeric_dataset.values.astype(float)\n",
    "\n",
    "    reduced_data, variance, varianceAbsolute = PCA(dataset, 0.9)\n",
    "    \n",
    "    total_variance_original = np.sum(np.linalg.eigvals(np.cov(A, rowvar=False)))\n",
    "    relative_total_variance = variance / total_variance_original\n",
    "    \n",
    "    results.append({'Dataset': dataset_name, 'Dimensiones': reduced_data.shape[1], 'Varianza total absoluta': variance, 'Varianza Total Relativa': relative_total_variance})\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results1D = []\n",
    "results2D = []\n",
    "results3D = []\n",
    "\n",
    "for dataset_name, dataset in datasetsDictionary2.items():\n",
    "    numeric_dataset = dataset.select_dtypes(include=[float])\n",
    "    X = numeric_dataset.values.astype(float)\n",
    "\n",
    "    absolute_variance = np.diag(np.cov(dataset, rowvar=False, bias=False))\n",
    "\n",
    "    fig, axs = plt.subplots(2, figsize=(7, 9))\n",
    "    fig.suptitle(f'Reducción 1D y 2D para {dataset_name}')\n",
    "\n",
    "    varianceAbs = []\n",
    "    variances = []\n",
    "    relative_variances = []\n",
    "\n",
    "    cov_matrix = np.cov(dataset, rowvar=False, bias=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "    for i, n_components in enumerate([1, 2, 3]):\n",
    "        reduced_data, total_variance, varianceAbsoluta = PCA(dataset, n_components, eigenvectors)\n",
    "\n",
    "        if n_components == 1:\n",
    "            axs[n_components-1].scatter(reduced_data, np.zeros_like(reduced_data))\n",
    "        elif n_components == 2:\n",
    "            axs[n_components-1].scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "        else:\n",
    "            axs[n_components-1].scatter(reduced_data[:, 0], reduced_data[:, 1], reduced_data[:, 2])\n",
    "\n",
    "        relative_total_variance = total_variance / np.sum(eigenvalues)\n",
    "        \n",
    "        varianceAbs.append(varianceAbsoluta)\n",
    "        variances.append(total_variance)\n",
    "        relative_variances.append(relative_total_variance)\n",
    "\n",
    "    results1D.append({'Dataset': dataset_name,\n",
    "                      'Varianza Original (1D)': varianceAbs[0],\n",
    "                      'Varianza Reducida (1D)': variances[0],\n",
    "                      'Porcentaje Mantenido': relative_variances[0]*100})\n",
    "    results2D.append({'Dataset': dataset_name,\n",
    "                      'Varianza Original (2D)': varianceAbs[1],\n",
    "                      'Varianza Reducida (2D)': variances[1],\n",
    "                      'Porcentaje Mantenido': relative_variances[1]*100})\n",
    "    results3D.append({'Dataset': dataset_name,\n",
    "                      'Varianza Original (3D)': varianceAbs[2],\n",
    "                      'Varianza Relativa (3D)': variances[2],\n",
    "                      'Porcentaje Mantenido': relative_variances[2]*100})\n",
    "    \n",
    "results_df1 = pd.DataFrame(results1D)\n",
    "results_df2 = pd.DataFrame(results2D)\n",
    "results_df3 = pd.DataFrame(results3D)\n",
    "\n",
    "print('1 Dimension')\n",
    "display(results_df1)\n",
    "print('2 Dimension')\n",
    "display(results_df2)\n",
    "print('3 Dimension')\n",
    "display(results_df3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
