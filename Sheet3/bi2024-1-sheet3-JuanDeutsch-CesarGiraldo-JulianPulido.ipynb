{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juan Manuel Deutsch, Cesar Felipe Giraldo, Julian Felipe Pulido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio tenemos $X$ siendo la matriz base y $Σ$ como la matriz de covarianza de $X$\n",
    "\n",
    "Tambien tenemos los componentes principales $u_1, u_2,...,u_n$, que son los autovectores o vectores propios de $Σ$ que tienen varianzas asociadas $u_i^TΣu_i$, donde $u_i^T$ es la transpuesta del vector propio $u_i$\n",
    "\n",
    "Ahora tendremos una tranformación lineal de los datos $X$, siendo $AX$ donde $A$ es una transformación $m x m$\n",
    "\n",
    "Ahora con la matriz transformada tenemos la matriz de covarianza transformada $AΣA^T$ cuyos componentes principales estan dados por $v_1, v_2,...,v_n$ estos siendo los autovectores cuyas varianza asociadas estan dadas por $v_i^TAΣA^Tv_i$\n",
    "\n",
    "Teniendo en cuenta que necesitamos demostrar los componentes principales y como cambian tenemos con relacion a $AX$ y $X$\n",
    "\n",
    "$u_1$ siendo componente principal cuya varianza asociada es $u_i^TΣu_i$ y siendo $v_1$ el componente principal cuya varianza asociada es $v_i^TAΣA^Tv_i$\n",
    "\n",
    "Teniendo en cuenta que $u_1$ y $v_1$ estan relacionados por una transformación lineal tenemos que $v_1 = Tu_1$ para una matriz T\n",
    "\n",
    "Ahora, se puede escribir la varianza asociada de $v_1$ como:\n",
    "\n",
    "$$v_1^TAΣA^Tv_1 = (Tu_1)^TAΣA^T(Tu_1) = u_1^TT^TAΣA^TTu_1$$\n",
    "\n",
    "Dado que $T$ es una matriz de transformacion lineal, $T^TAΣA^TT$ es otra matriz de covarianza $Σ'$\n",
    "\n",
    "Por lo tanto ahora tenemos:\n",
    "\n",
    "$$v_1^TAΣA^Tv_1 = u_1^TΣ'u_1$$\n",
    "\n",
    "Concluyendo con esta demostración, tenemos que la varianza asociada $v_1$ depende de la matriz de covarianza $Σ'$ que es una version transformada de la matriz de covarianza original $Σ$, por lo cual, $Σ'$ es diferente a $Σ$, entonces la varianza asociada a estos van a ser diferentes, por lo que un valor propio esta ligado conjunto de datos original y el otro valor propio esta ligado al conjunto transformado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta demostracion tenemos $D$, como una matriz $f x c$ donde $f$ son filas y $c$ son columnas, ademas de $D'$ que es la matriz transformada con una transformación afin a una o mas columnas\n",
    "\n",
    "Iniciando calculamos la matriz de correlación $corr(D)$\n",
    "$$corr(D) = 1/n (D - D_u)^T(D - D_u)$$\n",
    "Donde $D_u$ es la media de cada columna de $D$\n",
    "De acuerdo a eso se los componentes principales por medio de $corr(D)$\n",
    "\n",
    "Ahora realizamos el mismo proceso con la tranformación afin, que seria del siguiente modo:\n",
    "\n",
    "$$D' = A*D+b$$\n",
    "\n",
    "Donde $A$ es la transformación lineal y $b$ es el desplazamiento\n",
    "\n",
    "Ahora calculamos la correlacion \n",
    "\n",
    "$$corr(D') = 1/n (D' - D'_u)^T(D' - D'_u)$$\n",
    "\n",
    "Donde ahora $D'_u$ es la media de la matriz transformada, donde las columnas de $D'$ se ven afectadas por la transformación afin\n",
    "\n",
    "Pero debemos tener en cuenta que para la covarianza los efectos de una transformación afin se dan de la siguiente forma:\n",
    "\n",
    "$$cov(D') = A^Tcov(D)A$$\n",
    "\n",
    "No obstante, estamos tratando es con matrices de correlación, las cuales son versiones escaladas de las matrices de covariaza, por lo cual la matriz de covarianza tambien afecta a la matriz de correlación con la misma transformación afin\n",
    "\n",
    "Por lo cual los autovectores o vectores propios de $corr(D')$ son los mismos que $corr(D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[-0.81649658  0.57735027]\n",
      " [ 0.40824829  0.57735027]\n",
      " [ 0.40824829  0.57735027]]\n",
      "[[-2.22044605e-16 -1.03923048e+01]\n",
      " [-1.11022302e-16 -5.19615242e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 1.11022302e-16  5.19615242e+00]\n",
      " [ 2.22044605e-16  1.03923048e+01]]\n",
      "67.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def PCA(D, red):\n",
    "\n",
    "    mu = np.mean(D, axis=0)\n",
    "\n",
    "    Z = D - mu\n",
    "    \n",
    "    cov_matrix = np.cov(Z, rowvar=False, bias=False)\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    \n",
    "    if red < 1:\n",
    "\n",
    "        total_variance = np.sum(eigenvalues)\n",
    "        variance_top = red * total_variance\n",
    "        \n",
    "        cumulative_variance = 0\n",
    "        r = 0\n",
    "        for i in range(len(eigenvalues)):\n",
    "            cumulative_variance += eigenvalues[i]\n",
    "            if cumulative_variance >= variance_top:\n",
    "                r = i + 1\n",
    "                break\n",
    "    else:\n",
    "\n",
    "        r = red\n",
    "    \n",
    "    print(r)\n",
    "  \n",
    "    top_eigenvectors = eigenvectors[:, :r]\n",
    "\n",
    " \n",
    "    print(top_eigenvectors)\n",
    "    \n",
    "    A = np.dot(Z, top_eigenvectors)\n",
    "    \n",
    "    v = np.sum(eigenvalues[:r])\n",
    "    \n",
    "    return A, v\n",
    "\n",
    "#Prueba\n",
    "\n",
    "D = np.array([[1, 2, 3],\n",
    "               [4, 5, 6],\n",
    "               [7, 8, 9],\n",
    "               [10, 11, 12],\n",
    "               [13, 14, 15]])\n",
    "\n",
    "# Aplicamos PCA para reducir a 2 dimensiones conservando el 50% de la varianza\n",
    "A, variance = PCA(D, 0.5)\n",
    "print(A)\n",
    "print(variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv(\"amazon.csv\")\n",
    "bndf = pd.read_csv(\"baseball_numeric.csv\")\n",
    "fdf = pd.read_csv(\"fabert.csv\")\n",
    "idf = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "adf2 = adf.drop('Class', axis=1)\n",
    "fdf2 = fdf.drop('class', axis=1)\n",
    "idf2 = idf.drop('species', axis=1)\n",
    "\n",
    "#Quitamos las columnas que no tienen valores numericos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
